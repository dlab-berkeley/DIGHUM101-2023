{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning and text data\n",
    "\n",
    "If we have a corpus of texts, we first must preprocess those texts into a format the algorithms can understand. This usually means converting the representation of our text into numbers! We will do this using Scikit-learn, a popular Machine Learning library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "\n",
    "`CountVectorizer` will help us quickly tokenize text, learn its vocabulary, and encode the text as a vector for use in machine learning. This is often referred to as document encoding. \n",
    "\n",
    "Note that many ML operations in Scikit-learn is often a two-part process:\n",
    "\n",
    "### Fit -> Transform\n",
    "\n",
    "This means slightly different things based on the kind of ML algorithm you're using. Here, when dealing with document encoding, *fitting* means to tokenize texts and learning the vocabulary, while *transforming* means to (yes) transform the texts into vectors of numbers based on the encoding of that vocabulary. Our *model* will be this transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This is the second document.\",\n",
    "    \"And the third one.\",\n",
    "    \"Here we go with the fourth document?\"\n",
    "    ]\n",
    "\n",
    "# Define an empty bag (of words)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the .fit method to tokenize the text and learn the vocabulary\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Print the vocabulary. Note: this yields a dictionary. What are the keys and values? \n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've `fit` our texts to a vocabulary, we can `transform` our texts into a vector on the basis of the frequency (count) of each word that occurs in the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view our document term matrix as a sparse array, where each row is a document and each column is a vocabulary word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document term matrix\n",
    "\n",
    "A [document term matrix](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) displays term frequencies that occur across a collection of documents. We want to encode the documents into a matrix to represent the frequencies of each vocabulary word across the documents.\n",
    "\n",
    "However, representing our document-term matrix as a matrix where most values are 0 can end up costing us a lot of computing power. Performing operations across such a matrix may take a long time. So `sklearn` typically uses an alternate data structure to represent the data. Have a look:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the documents\n",
    "print(vector) \n",
    "print(vector.shape)\n",
    "print(type(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representation above is called a *compressed sparse row (csr) matrix*. A list of tuples is stored with each tuple containing the row and column index, and the value. It's an array that doesn't store the 0 values, just those where a token appears.\n",
    "\n",
    "The column headers of this list could read **(document number, vocabulary word), frequency**.\n",
    "\n",
    "*Note: the terms \"sparse\" and \"dense\" are often confused because they mean different thing in different contexts. In numerical analysis, a sparse matrix/array is one in which most of the elements are zero, while in dense matrices, most elements are non-zero (see [here](https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html) and [here](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/)). However, in programming contexts, a sparse matrix is one in which the elements are not sequential, and they don't always start at 0 (while dense matrices do). [It all depends on how you define them](https://ask.sagemath.org/question/10554/is-it-a-sparse-matrix-or-dense-matrix/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the arrays in the above cell. \n",
    "# In which documents does \"and\" appear? \n",
    "# What about \"document\"? What about \"the\"?\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this tell us? \n",
    "vectorizer.transform(['my new document is this']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also take a look at our document-term matrix as a matrix where each row is a document and each column is a vocabulary word. We do this by calling the `.toarray()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the kind of model we are building here does not take word order into account. It simply counts them per document! Put differently, `Countvectorizer` creates a **bag of words model**, which classifies a text by turning it into a \"bag\" of words to normalize and count them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams\n",
    "\n",
    "In addition to uni-grams, using bigrams can be useful to preserve some ordering information. Here we can look at two (bi) or three (tri) or four (quad) or more words at a time! \n",
    "\n",
    "> NOTE: **`ngram_range=(1,2)`** will get you bigrams, **`ngram_range=(1,3)`** will get you tri-grams, **`ngram_range=(1,4)`** will get you quad-grams, etc. \n",
    "\n",
    "> **`token_pattern=r'\\b\\w+\\b'`** is standard regex code to separate words.\n",
    "\n",
    "We could also add many other parameters, such as `stop_words='english'` to add a stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a bigram bag of words \n",
    "bigram_vectorizer = CountVectorizer(ngram_range = (1,2),\n",
    "                                    token_pattern = r'\\b\\w+\\b')\n",
    "bigram_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the bigram bag of words\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bigrams. Are cool!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply this idea to our `corpus` variable from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus transformation\n",
    "bigram_array = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "print(bigram_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are feature names? The column names! The rows are our documents :) \n",
    "print(bigram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that these counts are not word counts, but refer to the index of the word in the vocab\n",
    "bigram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review - Exploring Data with Pandas\n",
    "\n",
    "For this next section we will use a dataset called `music_reviews.csv` (collected from [Metacritic](https://www.metacritic.com/)), which includes album reviews from well-known music magazines. \n",
    "\n",
    "Let's first explore the data. This serves not only as a basic informative purpose, but also to ensure there are not any glaring errors. Our data includes both the actual review (in the \"body\" column) and the numeric score, so we can start by exploring the latter.\n",
    "\n",
    "First, what genres are in this dataset, and how many reviews in each genre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"../../Data/music_reviews.csv\", sep = \"\\t\")\n",
    "print(reviews.shape)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who were the artists?\n",
    "reviews.artist.value_counts().head(20)\n",
    "\n",
    "# or\n",
    "\n",
    "# reviews['artist'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who were the reviewers?\n",
    "reviews['critic'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the distribution of review scores like?\n",
    "reviews['score'].plot(kind='hist', \n",
    "                      bins = 50, \n",
    "                      figsize = (6, 3)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember .groupby? It allows us to group the scores by genre!\n",
    "reviews_grouped_by_genre = reviews.groupby('genre')\n",
    "\n",
    "# Now let's get the mean for all scores, sorting in descending order  \n",
    "reviews_grouped_by_genre['score'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, let's make barplots for the number of reviews by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequencies (counts) for the number of reviews by genre\n",
    "reviews['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this to a data frame\n",
    "gen = pd.DataFrame(reviews['genre'].value_counts())\n",
    "\n",
    "gen = gen.reset_index()\n",
    "gen.columns =['GENRE', 'COUNT']\n",
    "\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "gen_fig = sns.barplot(x = 'COUNT', \n",
    "                      y = 'GENRE', \n",
    "                      data = gen, \n",
    "                      orient = 'h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also make barplots for average review score by genre and boxplots for the review scores by genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_review = reviews.groupby('genre')['score'].mean().sort_values()\n",
    "mean_review.plot.barh(rot = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots of average score by genre\n",
    "sns.boxplot(x = \"score\", y = \"genre\", data = reviews);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: remember that exploring your data with basic summary statistics and visualizations is a good first step before anything more complex!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "[Term frequency–inverse document frequency (TFIDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) can be thought of as an extension of `CountVectorizer`. However, instead of counting words, TFIDF identifies unique words within and across documents. We'll talk more about what it is later. First, let's recap a bit of data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Python's scikit-learn package again. We'll use `Counvectorizer` as we did before, but we'll also use a word weighting technique called TF-IDF (term frequency inverse document frequency) to identify important and discerning words within this dataset with Pandas.\n",
    "\n",
    "We ask the question: **what words distinguish reviews of Rap albums, Indie Rock albums, and Jazz albums?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is going on here?\n",
    "def remove_digits(comment):\n",
    "    return ''.join([ch for ch in comment if not ch.isdigit()])\n",
    "\n",
    "reviews['body_without_digits'] = reviews['body'].apply(remove_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first body entry\n",
    "reviews[\"body\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View that same body entry - but without digits! What happened?\n",
    "list(reviews[\"body_without_digits\"])[190]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `CountVectorizer` revisited\n",
    "\n",
    "Let's first revisit `CountVectorizer` and see what kind of vocabulary we are dealing with in the music reviews \"body_without_digits\" column. Whoa, that is a lot of words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_bow = cv.fit_transform(reviews['body_without_digits'])\n",
    "print(cv_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is actually called Compressed Sparse Format and is useful because we can save huge document term matrices in this format - but it is difficult to look at for a human. Let's convert it to a format we are more familiar with - a dataframe. We call this object \"dtm\" as it is a document-term matrix - a matrix with all the terms and their counts in all the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame(cv_bow.toarray(), columns=cv.get_feature_names_out(), index=reviews.index)\n",
    "print(dtm.shape)\n",
    "\n",
    "dtm.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note most of the counts are 0: it's a sparse matrix. We're spending a lot of memory resources on zero values, which do not contain any useful information. This is why we use other representations such as Coordinate Lists or Compressed Sparse Matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at just the first row in its entirety\n",
    "# Now do a command + f / control + f search for the number 1\n",
    "pd.set_option('display.max_rows', None)\n",
    "dtm.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can we do with a DTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quickly identify the most frequent words:\n",
    "dtm.sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the most infrequent words:\n",
    "dtm.sum().sort_values().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the average number of times each word is used in a review:\n",
    "dtm.mean().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF scores\n",
    "\n",
    "How to find distinctive words in a corpus is a long-standing question in text analysis. Today, we'll learn one simple approach to this: TF-IDF. The idea behind words scores is to weight words not just by their frequency, but by their frequency in one document compared to their distribution across all documents. Words that are frequent, but are also used in every single document, will not be distinguishing. We want to identify words that are unevenly distributed across the corpus using TF-IDF. By offsetting the frequency of a word by its document frequency (the number of documents in which it appears) will in theory filter out common terms such as 'the', 'of', and 'and'.\n",
    "\n",
    "Traditionally, the inverse document frequency is calculated as such:\n",
    "\n",
    "**idf_word1 = number_of_documents / number_documents_with_word1**\n",
    "\n",
    "so TF-IDF is:\n",
    "\n",
    "**tfidf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)**\n",
    "\n",
    "You can, and often should, normalize the numerator (so there's no bias for longer or shorter documents). Otherwise, long documents (with lots of words) would affect the TF-IDF scores.\n",
    "\n",
    "**tfidf_word1 = (word1_frequency_document1 / word_count_document1) * (number_of_documents / number_document_with_word1)**\n",
    "\n",
    "We can calculate all of this manually, but scikit-learn has a built-in function to do so. This function also uses log frequencies, so the numbers will not correspond excactly to the calculations above. We'll use the [scikit-learn calculation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDFVectorizer Function\n",
    "\n",
    "To do so, we simply do the same thing we did before with `CountVectorizer`, but instead we use the function `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "tfidfvec = TfidfVectorizer(min_df=2,lowercase=True,stop_words=stop)\n",
    "\n",
    "tfidf_bow = tfidfvec.fit_transform(reviews['body_without_digits'])\n",
    "print(tfidf_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each word in each review now has a TF-IDF score attached to it.\n",
    "\n",
    "Again, let's turn this into a (sparse) Pandas DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(tfidf_bow.toarray(), columns=tfidfvec.get_feature_names_out(), index=reviews.index)\n",
    "tfidf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we still have a lot of zeroes – that is, documents in which certain words don't appear at all (and thus don't receive a TF-IDF score).\n",
    "\n",
    "What are the words with the highest TF-IDF score across all documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the 20 words with highest tf-idf weights:\n",
    "tfidf.max().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We have successfully identified content words, without removing stop words. What else do you notice about this list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Distinctive Words\n",
    "\n",
    "What can we do with this? These scores are best used when you want to identify distinctive words for individual documents, or groups of documents, compared to other groups or the corpus as a whole. To illustrate this, let's compare three genres and identify the most distinctive words by genre.\n",
    "\n",
    "First we add in a column of genre: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf['genre_'] = reviews['genre']\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the words with the highest tf-idf weight for each genre. We'll create three dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rap = tfidf[tfidf['genre_'] == 'Rap']\n",
    "indie = tfidf[tfidf['genre_'] == 'Indie']\n",
    "jazz = tfidf[tfidf['genre_'] == 'Jazz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a quick look\n",
    "rap.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: max() gets the max value for each row \n",
    "# numeric_only() excludes the \"genre_\" column\n",
    "rap.max(numeric_only=True).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indie.max(numeric_only=True).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz.max(numeric_only=True).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us? For instance, it might be interesting that \"authentic\" is typically used in rap reviews, as well as terms like \"tight\" and \"punch\". Meanwhile, indie is connected with words like \"likable\" and \"awesome\", and jazz with more technical terminology like \"minimalist\", \"innovative\" and \"descending\".\n",
    "\n",
    "In week 5 you will learn about topic modeling to see how machines can identify potentially abstract topics in text(s)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
