{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>DIGHUM101</b></center>\n",
    "<center>2-1: Text Preprocessing</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast review\n",
    "\n",
    "1. What is a data frame? \n",
    "2. What methods and other syntax can be used to subset rows and columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "\n",
    "1. Download a .txt file from Project Gutenberg and import it into Python\n",
    "2. Quick walkthrough of Hathi Trust Research Center (HTRC) resources\n",
    "3. Learn the basics of text preprocessing: \n",
    "    - Tokenization\n",
    "    - Punctuation removal\n",
    "    - Count words, unique words, and word frequencies\n",
    "    - Stop word removal\n",
    "    - Stemming/lemmatization\n",
    "    - Part of speech tagging\n",
    "    - Quick introduction to n-grams, skip-grams, and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module to remove punctuation from string library\n",
    "from string import punctuation\n",
    "print(punctuation)\n",
    "print(len(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module to count word frequencies\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module to help us remove stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy and trained model downloaded.\n",
    "# install spacy\n",
    "#!pip install spacy\n",
    "\n",
    "# Download a trained English model (small)\n",
    "# !python -m spacy download en_core_web_sm \n",
    "\n",
    "# Download the large model as well\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Gutenberg\n",
    "\n",
    "[Project Gutenberg](https://www.gutenberg.org/) has more than 60,000 texts for you to download. Be sure to check out their [Terms of Use](https://www.gutenberg.org/wiki/Gutenberg:Terms_of_Use). You can find many .txt files here that are in the public domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it! Search for a book, download it, copy it to your working directory, and import it.\n",
    "\n",
    "## YOUR CODE HERE\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../Data/\")\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HOW TO IMPORT dracula.txt?\n",
    "dracula = open(\"dracula.txt\").read()\n",
    "print(dracula[501:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hathi Trust Research Center (HTRC)\n",
    "\n",
    "Check out the [HTRC](https://www.hathitrust.org/) and learn about their many [collections tools](https://www.hathitrust.org/htrc_collections_tools) and the [Python library](https://github.com/htrc/htrc-feature-reader) to connect to the API. The [Analytics](https://analytics.hathitrust.org/) website gives you access to many canned features if you don't want to mess with the Python code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing: Strings in depth\n",
    "\n",
    "Text preprocessing is an essential first step to coding and understanding machine learning algorithms. For machine learning portions of this course, we will focus on [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) models, namely [document-term](https://en.wikipedia.org/wiki/Document-term_matrix) and [term frequency-inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) matrices from the [sklearn library](https://scikit-learn.org/stable/).\n",
    "\n",
    "Text preprocessing/pattern matching can be further enhanced through use of [regular expressions](https://docs.python.org/2/library/re.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![borges](../../Img/borges_1921.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borges = '''In the fullness of the years, like it or not,\n",
    "a luminous mist surrounds me, unvarying, \n",
    "that breaks things down into a single thing,\n",
    "colorless, formless. Almost into a thought. \n",
    "The elemental, vast night and the day\n",
    "teeming with people have become that fog\n",
    "of constant, tentative light that does not flag,\n",
    "\n",
    "and lies in wait at dawn. I longed to see\n",
    "just once a human face. Unknown to me\n",
    "the closed encyclopedia, the sweet play\n",
    "in volumes I can do no more than hold, \n",
    "the tiny soaring birds, the moons of gold.\n",
    "Others have the world, for better or worse; \n",
    "I have this half-dark, and the toil of verse.'''\n",
    "\n",
    "\n",
    "print(type(borges))\n",
    "print()\n",
    "print(borges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the triple quotes do in the assignment of borges above?\n",
    "\n",
    "# Also, make a copy to preserve the original borges variable\n",
    "poem = borges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into _something_ - often words. Each word is called a \"token\" and a word such as \"the\" might adhere to multiple tokens of \"the\" within a text based on its capitalization, punctuation, etc.\n",
    "\n",
    "The `.split()` method allows us to split the text based on some sort of separator. The default is blank and will split on the blank spaces between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the string into a list of strings (single words)\n",
    "print(poem.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count words\n",
    "\n",
    "Jump in! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many characters in poem?\n",
    "len(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words?\n",
    "len(poem.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lines?\n",
    "len(poem.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many periods? \n",
    "# Should this be equal to the number of sentences in the cell below?\n",
    "poem.count(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... but how many sentences? Why is this different from the number of periods?\n",
    "len(poem.split(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many stanzas?\n",
    "len(poem.split(\"\\n\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At which index does the word \"me\" first appear?\n",
    "# .find() is \"forward search\"\n",
    "poem.find(\"me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .index works as well\n",
    "poem.index(\"me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that .find does not throw an error when an element is not found (but .index does)\n",
    "poem.find(\"kangaroo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At which index does the word \"me\" last appear?\n",
    "# .rfind() starts at the highest index and works in reverse\n",
    "poem.rfind(\"me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count _unique_ words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique words?\n",
    "# \"Casting\" our list into a set\n",
    "len(set(poem.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why two less unique words when we convert all the text to lower?\n",
    "len(set(poem.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique words\n",
    "print(set(poem.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of data structure is this? \n",
    "type(set(poem.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is this different from .lower()?\n",
    "len(set(poem.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation removal \n",
    "\n",
    "Remember how we imported that nice string of English punctuation in the first cell of this notebook? We could manually remove all of the punctuation using the .replace method, but this would get old fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many characters\n",
    "len(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace periods with nothing\n",
    "del_periods = poem.replace(\".\", \"\")\n",
    "del_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what if you have tons of text and don't know exactly what punctuation is present? A quick comprehension can help us remove all the punctuation from dirty, i.e. !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~)\n",
    "\n",
    "> NOTE: You will learn more about custom functions, for loops, list comprehensions, and lambda functions starting in week 3. The reason we are glossing over them now is so that you focus on **what is possible** for planning your individual projects instead of getting lost and frustrated in the nuances of the Python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop\n",
    "for char in punctuation:\n",
    "    poem = poem.lower().replace(char, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Punctuation is gone! \n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize poem into single words\n",
    "tokens = poem.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the ten most common words (stopwords included)\n",
    "freq = Counter(tokens)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop word removal\n",
    "\n",
    "[Stop words](https://en.wikipedia.org/wiki/Stop_words) are the most common words in a language, and may or may not add information about the content of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension (we also saw for converting to datetime in \"2-1_pandas.ipynb\")\n",
    "no_stops = [word for word in tokens if word not in stopwords.words('english')]\n",
    "print(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as the following:\n",
    "no_stops = []\n",
    "for word in tokens:\n",
    "    if word not in stopwords.words('english'):\n",
    "        no_stops.append(word)\n",
    "print(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq2 = Counter(no_stops)\n",
    "freq2.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "One common problem with standardizing text is standardizing all parts of a word to its root, stem, or prefix. This is useful as it allows us to analyze word meaning without having to pour over separate inflectional forms of a word. It also speeds up the process as we have fewer words.\n",
    "\n",
    "[What's the difference between stemming and lemmatizing?](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)\n",
    "\n",
    "\"**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. \n",
    "\n",
    "If confronted with the token \"saw\", **stemming** might return just \"s\", whereas **lemmatization** would attempt to return either \"see\" or \"saw\" depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma.\" See [this post](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) for more information.\n",
    "\n",
    "So what do we do? Use pretrained models from [spaCy](https://spacy.io/)! It does all the tokenization and lemmatization for you.\n",
    "\n",
    "[Read more about spaCy's pretrained models](https://spacy.io/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing with spaCy\n",
    "\n",
    "Let's start by loading the trained model. We don't need the Named Entity Recognition (NER) or the text classification capabilities of the model, so we don't load them to make everything faster. If we wanted to lemmatize a language other than English, we would just need to download a trained model for that language using spaCy and change the 'en_core_web_sm' to the name of that model. Everything else would be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small pretrained model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
    "print(type(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy expects a string as input, so let's use .join to force our list into a string  \n",
    "words = ' '.join(tokens)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(words)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now get the lemma of any word using the .lemma_ attribute\n",
    "doc[5].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...Or even the part of speech\n",
    "doc[5].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that spaCy also has its own stopwords list\n",
    "nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the [spaCy documentation](https://spacy.io/api/token#attributes) for more information about all the linguistic features that spaCy allows you to access as attributes.\n",
    "\n",
    "Now, let's create a function that takes in a list of tokens and lemmatizes it using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our function\n",
    "def lemmatize(tokens):\n",
    "    \"\"\"Return the lemmas for each word in `tokens`.\"\"\"\n",
    "    \n",
    "    # spacy models operate on strings, not lists, so we turn the tokens back into\n",
    "    # a string of words\n",
    "    words = ' '.join(tokens)\n",
    "    \n",
    "    # this line does all sorts of processing, including the lemmatization.\n",
    "    # `doc` will be like a list of tokens that we can iterate over\n",
    "    doc = nlp(words)\n",
    "    \n",
    "    # each token in `doc` holds information about that token. The `lemma_`\n",
    "    # attribute holds the lemma of that token represented as a string. For\n",
    "    # performance reasons, the `lemma` (without the trailing underscore) holds\n",
    "    # an integer representation of the token, that we'll rarely ever need.\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemmas = lemmatize(tokens)\n",
    "print(lemmas)\n",
    "\n",
    "# Notice that spacy lemmatizes pronouns (e.g. \"you\", \"I\", \"your\") in a funny way.\n",
    "# It just tells us that they are pronouns, rather than giving us something like\n",
    "# \"your\" -> \"you\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams, skip-grams, and BERT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you interested in tokenizing more than just single words for the purpose of increasing \"context\"? [n-grams](https://en.wikipedia.org/wiki/N-gram) are \"contiguous sequence of n items from a given sample of text or speech.\"\n",
    "\n",
    "[Check out this clever solution for n-gramizing text](https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams)\n",
    "\n",
    "Do you want even more context? We will learn more about [skip-grams](https://en.wikipedia.org/wiki/Word2vec) and [BERT](https://www.searchenginejournal.com/google-bert-update/332161/#close) later in this course. \n",
    "\n",
    "\n",
    "\"The skip-gram architecture weighs nearby context words more heavily than more distant context words.\"\n",
    "\n",
    "\"The BERT algorithm (Bidirectional Encoder Representations from Transformers) is a deep learning algorithm related to natural language processing. It helps a machine to understand what words in a sentence mean, but with all the nuances of context.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
