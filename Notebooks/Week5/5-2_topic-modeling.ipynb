{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>DIGHUM101</b></center>\n",
    "<center>5-2: Topic Modeling</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives \n",
    "\n",
    "- Preprocessing text data\n",
    "- Create an LDA topic model using `sklearn` and `gensim`\n",
    "- Visualize and interpret a topic model using `pyLDAvis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install new libraries if needed\n",
    "\n",
    "# !pip install wordcloud\n",
    "# !pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from collections import Counter # Count most common words\n",
    "%matplotlib inline\n",
    "import nltk # natural language toolkit\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "import pyLDAvis.sklearn # visualize our topic models!\n",
    "import re # regular expressions\n",
    "# Preprocessing\n",
    "import gensim\n",
    "# Algorithms (unsupervised)\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# Tools to create our DTMs\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "# Visualize word clouds \n",
    "from wordcloud import WordCloud\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "There are many topic modeling algorithms, but we'll start with [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). This is a standard **unsupervised** machine learning text-mining tool that can be used to discover abstract \"topics\" contained within texts.\n",
    "\n",
    "See [this cool animation](https://en.wikipedia.org/wiki/File:Topic_model_scheme.webm) on Wikipedia to get an idea about topic modeling works.\n",
    "\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "- **Topic Modeling:** A general class of statistical models that uncover abstract topics within a text. It uses the co-occurrence of words within documents, compared to their distribution across documents, to uncover these abstract themes. The output is a list of weighted words, which indicate the subject of each topic, and a weight distribution across topics for each document.\n",
    "    \n",
    "- **LDA:** Latent Dirichlet Allocation. A particular model for topic modeling. It does not take document order into account, unlike other topic modeling algorithms. Also see word2vec and BERT! (Week 5)\n",
    "\n",
    "Like the rest of this class, the goal is not to learn everything about topic modeling. Instead, this notebook will provide you with some starter code to run some simple models with the idea that you can use this base of knowledge to explore further. Use the `sklearn` help files, Stack Overflow, and Google searching to review and learn more about what the code is doing and how to go further. \n",
    "\n",
    "Can you make this code work for your own data? Can you tweak the parameters to get better output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe from individual text files\n",
    "\n",
    "You've gathered a bunch of text files, so now what? It is useful to get these files into a dataframe. Python does not make this terribly easy for the beginner, so use the boilerplate code below to help you.\n",
    "\n",
    "Let's concatenate the eleven text files in the \"Data/human-rights/\" folder into a dataframe so we can manipulate that text like we have seen in the previous few notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where am I?\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable with the file path for the directory containing the text files\n",
    "# Go two directories up (../../) \n",
    "# and into the Data directory\n",
    "# then into the human-rights subdirectory\n",
    "dir_path = os.listdir(\"../../Data/human-rights/\")\n",
    "\n",
    "# View the contents of this directory\n",
    "dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate an empty dictionary to store the filename and text as columns\n",
    "for_dataframe = {}\n",
    "\n",
    "# Loop through the directory of text files and open and read them\n",
    "for file in dir_path:\n",
    "    with open(\"../../Data/human-rights/\" + file, \"r\", encoding=\"utf-8\") as to_open:\n",
    "         for_dataframe[file] = to_open.read()\n",
    "            \n",
    "# Create and append the dataframe with two columns - the file name and the text itself\n",
    "human_rights = (pd.DataFrame.from_dict(for_dataframe, \n",
    "                                       orient = \"index\")\n",
    "                .reset_index().rename(index = str, \n",
    "                                      columns = {\"index\": \"File\", 0: \"Text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review - manipulate and explore text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out text of one row to make sure it looks okay...\n",
    "human_rights.iloc[0,1][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic preprocessing\n",
    "\n",
    "Preprocess the text! What else might you want to do that is not included here? Lemmatization? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights[\"Text_processed\"] = human_rights[\"Text\"].apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "human_rights[\"Text_processed\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gensim for preprocessing using .apply()\n",
    "processed = human_rights[\"Text\"].apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "# Stopword removal using NLTK stopword list and a lambda function\n",
    "stop = stopwords.words('english')\n",
    "no_stop = processed.apply(lambda x: [w for w in x if w not in stopwords.words('english')]) \n",
    "\n",
    "# Convert list back to str\n",
    "human_rights[\"Text_processed\"] = [' '.join(t) for t in no_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "human_rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights['Text_processed'][0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top-10 words\n",
    "\n",
    "hr_str = ' '.join(human_rights['Text_processed'].tolist())\n",
    "hr_tok = hr_str.split()\n",
    "hr_freq = Counter(hr_tok)\n",
    "\n",
    "# Print the 10 most common words\n",
    "hr_df = pd.DataFrame(hr_freq.most_common(10), columns = [\"Word\", \"Frequency\"])\n",
    "hr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv!\n",
    "human_rights.to_csv('../../Data/human_rights.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a BOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty bag (of words)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the .fit method to tokenize the text and learn the vocabulary\n",
    "vectorizer.fit(human_rights[\"Text_processed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the DTM\n",
    "\n",
    "Recall that a [document term matrix](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) displays term frequencies or TFIDF scores that occur across a collection of documents. We want to encode the documents into a [sparse matrix](https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html#:~:text=By%20definition%2C%20a%20sparse%20matrix,as%20a%20word%2Dcount%20vector.&text=Thus%2C%20if%20most%20of%20your,most%20likely%20sparse%20as%20well!) to represent the frequencies or TFIDF scores of each vocabulary word across the documents.\n",
    "\n",
    "Again, the column headers could read **(document number, term)   frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the documents\n",
    "vector = vectorizer.transform(human_rights[\"Text_processed\"])\n",
    "\n",
    "print(type(vector))\n",
    "print(vector.shape)\n",
    "print(vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View as a multidimensional array before converting to data frame\n",
    "# Rows are the documents, columns are the terms\n",
    "\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the terms\n",
    "\n",
    "vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a bigram bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we are entering regular expression as a token_pattern argument\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range = (1,2),\n",
    "                                    stop_words = \"english\",\n",
    "                                    token_pattern = r'\\b\\w+\\b', \n",
    "                                    min_df = 1)\n",
    "\n",
    "bigram_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze string in the bigram bag of words\n",
    "\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "vocab = analyze(hr_str)\n",
    "\n",
    "vocab[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 20 most commons\n",
    "freq = Counter(vocab)\n",
    "stop_df = pd.DataFrame(freq.most_common(20), columns = [\"Word\", \"Frequency\"])\n",
    "stop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a word cloud variable\n",
    "cloud = WordCloud(background_color = \"white\", \n",
    "                  max_words = 30, \n",
    "                  contour_width = 5, \n",
    "                  width = 600, height = 300, \n",
    "                  random_state = 4)\n",
    "\n",
    "# Process the word cloud\n",
    "cloud.generate(hr_str)\n",
    "\n",
    "# Visualize!\n",
    "cloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn about using [custom colors here](https://amueller.github.io/word_cloud/auto_examples/a_new_hope.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word frequencies in a horizontal bar plot\n",
    "\n",
    "sns.barplot(x = \"Frequency\",\n",
    "            y = \"Word\",\n",
    "            data = stop_df,\n",
    "            orient = \"h\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally! Fit the topic model\n",
    "\n",
    "The input to LDA should be a DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predetermine the number of topics\n",
    "\n",
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer to create the DTM, using some arguments to filter words!\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df = 0.9, # ignore terms that appear in more than 90% of the documents \n",
    "                                   max_features = 500) # using the 500 most-frequent words accross all documents \n",
    "\n",
    "# Fit\n",
    "cv = tf_vectorizer.fit_transform(human_rights[\"Text_processed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Check out this question](https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer) to learn more about the `max_df` and `min_df` arguments. \n",
    "\n",
    "Finally, let's run our LDA model! Remember that LDA is a probabilistic model that tries to estimate probability distributions for topics in documents and words in topics. We are using raw frequencies here but could also use TFIDF. This would increase the chance of rare words being sampled, making them have a stronger influence in topic assignment. Try it out if you feel like it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our LDA model (this might take a minute or two)\n",
    "lda = LatentDirichletAllocation(n_components = n_topics, \n",
    "                                max_iter = 20, # the maximum number of passes over the training data (aka epochs) \n",
    "                                random_state = 42) # setting random_state creates replicable results \n",
    "lda = lda.fit(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a function to print out the top words for each topic in a pretty way:\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #{}:\".format(topic_idx+1))\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the topics\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pyLDAvis.sklearn.prepare(lda_model = lda, \n",
    "                                 dtm = cv, \n",
    "                                 vectorizer = tf_vectorizer, \n",
    "                                 mds = \"tsne\") # method for dimensionality reduction - compare tsne to pcoa\n",
    "\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting PyLDAvis output\n",
    "- Similar topics should appear close together on the plot; dissimilar topics should appear far apart. \n",
    "- The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus.\n",
    "\n",
    "## Salience\n",
    "When no topic is selected in the plot on the left, the right bar chart shows the top-30 most **salient** terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics.\n",
    "\n",
    "## Probability Vs Exclusivity \n",
    "When you select a particular topic, this bar chart changes to show the top-30 most \"relevant\" terms for the selected topic. The relevance metric is controlled by the parameter λ, which can be adjusted with a slider above the bar chart:\n",
    "\n",
    "* Setting λ close to 1.0 (the default) will rank the terms according to their probability within the topic.\n",
    "* Setting λ close to 0.0 will rank the terms according to their \"distinctiveness\" or \"exclusivity\" within the topic. This means that terms that occur only in this topic, and do not occur in other topics.\n",
    "\n",
    "You can move the slider between 0.0 and 1.0 to weigh term probability and exclusivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `gensim`\n",
    "\n",
    "There's a lot of different packages in Python that allow you to do topic modeling. `gensim` is another. Let's run a similar topic model just to inspect the differences, and to see how much these choices matter for the output you get!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "prep = [t.split() for t in human_rights[\"Text_processed\"]]\n",
    "\n",
    "# Create Dictionary - like \"fit\" using sklearn\n",
    "dictionary = corpora.Dictionary(prep)\n",
    "\n",
    "# filter extremes and assign new ids\n",
    "dictionary.filter_extremes(no_above=0.9, keep_n=500)\n",
    "dictionary.compactify() \n",
    "\n",
    "# Create Document-Term Matrix of our whole corpus (like \"transform\" using sklearn) \n",
    "corpus = [dictionary.doc2bow(text) for text in prep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect a random ID in the dict for some word\n",
    "dictionary.token2id['achieved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first document, first 10 word IDs and their count\n",
    "corpus[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second document, first 10 word IDs and their count\n",
    "corpus[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "lda_gensim = LdaModel(corpus=corpus,   # stream of document vectors or sparse matrix of shape\n",
    "            id2word=dictionary,       # mapping from word IDs to words (for determining vocab size)\n",
    "            num_topics=5,            # amount of topics\n",
    "            random_state=42,         # seed to generate random state; useful for reproducibility\n",
    "            passes=20,                 # amount of iterations/epochs \n",
    "            per_word_topics=False)    # computing most-likely topics for each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "panel = gensimvis.prepare(lda_gensim, corpus, dictionary, mds=\"tsne\")\n",
    "\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "1. What is a topic in LDA? \n",
    "2. What is the relevance metric lambda in the pyLDAvis plot?\n",
    "3. What do you know about the eleven human rights documents we used to do this exercise? \n",
    "4. Why are all these topics similar in size in the left plot?\n",
    "5. Plug in your own data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2\n",
    "\n",
    "Read up on LDA and its visualizations by clicking the below links:\n",
    "- https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/\n",
    "- http://www.cs.columbia.edu/~blei/papers/ChaneyBlei2012.pdf\n",
    "- https://shravan-kuchkula.github.io/topic-modeling/#lda-results\n",
    "- https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html\n",
    "- http://vis.stanford.edu/files/2012-Termite-AVI.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
